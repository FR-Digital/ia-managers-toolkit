# Instructions Sp√©cifiques - Repo: ai-evaluation-pratique

**R√©f√©rence g√©n√©rique :** Voir fichier `instructions` pour les standards communs.

---

## Informations Repo

**Nom :** `ai-evaluation-pratique`
**Tagline :** "Savoir si votre IA fonctionne bien, sans √™tre data scientist"
**Priorit√© :** üî•üî• HAUTE
**URL :** `https://github.com/lafabriq/ai-evaluation-pratique`

---

## Objectif Sp√©cifique

Donner aux managers les outils pour √©valuer la qualit√© d'un syst√®me IA de mani√®re simple et business-oriented, en r√©pondant aux 3 questions :
1. Est-ce que √ßa marche ?
2. Est-ce que c'est utilis√© ?
3. Est-ce que √ßa rapporte ?

---

## Structure Compl√®te

```
ai-evaluation-pratique/
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ SOURCES.md
‚îÇ
‚îú‚îÄ‚îÄ frameworks/
‚îÇ   ‚îú‚îÄ‚îÄ 3-niveaux-evaluation.md        ‚Üê PRIORIT√â 1
‚îÇ   ‚îú‚îÄ‚îÄ metriques-business-vs-tech.md
‚îÇ   ‚îî‚îÄ‚îÄ quand-arreter-projet.md
‚îÇ
‚îú‚îÄ‚îÄ guides/
‚îÇ   ‚îú‚îÄ‚îÄ organiser-tests-utilisateurs.md
‚îÇ   ‚îú‚îÄ‚îÄ questions-equipe-technique.md   ‚Üê PRIORIT√â 2
‚îÇ   ‚îî‚îÄ‚îÄ criteres-go-production.md
‚îÇ
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ grille-evaluation.xlsx          ‚Üê PRIORIT√â 3
‚îÇ   ‚îú‚îÄ‚îÄ rapport-qualite.md
‚îÇ   ‚îî‚îÄ‚îÄ feedback-utilisateur.md
‚îÇ
‚îú‚îÄ‚îÄ checklists/
‚îÇ   ‚îú‚îÄ‚îÄ qualite-minimum.md
‚îÇ   ‚îî‚îÄ‚îÄ pre-production.md
‚îÇ
‚îî‚îÄ‚îÄ examples/
    ‚îú‚îÄ‚îÄ evaluation-chatbot.md
    ‚îî‚îÄ‚îÄ evaluation-analyse-sentiment.md
```

---

## Contenu Prioritaire √† Cr√©er

### PRIORIT√â 1 : Framework "3 Niveaux d'√âvaluation"

**Fichier :** `frameworks/3-niveaux-evaluation.md`

**Concept cl√© :** √âvaluer une IA en 3 niveaux s√©quentiels (ne pas passer au suivant sans valider le pr√©c√©dent)

**Structure du contenu :**

```markdown
# Framework d'√âvaluation IA en 3 Niveaux

## Le Principe
3 questions dans l'ordre :
1. Niveau 1 : Est-ce que √ßa marche ? (Tests techniques)
2. Niveau 2 : Est-ce que c'est utilis√© ? (Engagement)
3. Niveau 3 : Est-ce que √ßa rapporte ? (ROI)

## Niveau 1 : Est-ce que √áa Marche ? üîß
- Objectif : V√©rifier fonctionnement technique
- M√©thode : Test des 100 Cas (70 normaux, 20 limites, 10 pi√®ges)
- Scoring : ‚úÖ Correcte = 1pt, ‚ö†Ô∏è Partielle = 0.5pt, ‚ùå Fausse = 0pt
- Seuils : >80% valid√©, 60-80% am√©liorer, <60% stop
- M√©triques √† demander : Pr√©cision >70%, Temps <5s, Erreur <10%, Dispo >95%
- Checklist de validation

## Niveau 2 : Est-ce que les Utilisateurs l'Utilisent ? üë•
- Objectif : V√©rifier adoption et satisfaction
- M√©thode : Beta Test 2-4 semaines avec 20-50 utilisateurs
- M√©triques quantitatives : utilisation/jour, temps session, abandon, thumbs up/down
- M√©triques qualitatives : satisfaction 1-5, entretiens
- Seuils : Adoption >50%, Satisfaction >3.5/5, Thumbs up >60%, Abandon <30%
- Signaux d'alerte et causes possibles
- Checklist de validation

## Niveau 3 : Est-ce que √áa a un Impact Business ? üí∞
- Objectif : Mesurer valeur business
- M√©thode : A/B Test sur 1-3 mois
- KPIs √† comparer : temps r√©ponse, satisfaction, volume trait√©, r√©solution 1er contact
- Calcul ROI : (Gains - Co√ªts) / Co√ªts √ó 100
- Seuils : ROI >0%, Payback <18 mois, KPI +10%
- Checklist de validation

## Quand Arr√™ter un Projet
- üî¥ Niveau 1 √©choue (<60%) ‚Üí Stop
- üü° Niveau 2 √©choue (<30% adoption) ‚Üí Am√©liorer 1-2 mois ou Stop
- üü¢ Niveau 3 d√©cevant (ROI <0%) ‚Üí Optimiser 3-6 mois ou Stop

## R√©sum√© Visuel (flowchart ASCII)
```

**Exemples √† inclure :**
- Chatbot SAV : cas normal, limite, pi√®ge avec scoring
- Tableau comparatif A/B test avec m√©triques r√©elles
- Calcul ROI concret (180k‚Ç¨ gains - 105k‚Ç¨ co√ªts = 71% ROI)

---

### PRIORIT√â 2 : Guide "Questions √† Poser √† Votre √âquipe Technique"

**Fichier :** `guides/questions-equipe-technique.md`

**Contenu :** 30 questions business-friendly organis√©es par cat√©gorie

**Cat√©gories sugg√©r√©es :**
1. Questions sur la performance (5-6 questions)
2. Questions sur la fiabilit√© (5-6 questions)
3. Questions sur les donn√©es (5-6 questions)
4. Questions sur les co√ªts (5-6 questions)
5. Questions sur les risques (5-6 questions)

**Format pour chaque question :**
- La question en langage simple
- Ce que la r√©ponse devrait contenir
- Signal d'alerte si la r√©ponse est vague
- Exemple de bonne vs mauvaise r√©ponse

---

### PRIORIT√â 3 : Template Excel "Grille d'√âvaluation"

**Fichier :** `templates/grille-evaluation.xlsx`

**Onglets :**
1. **Test 100 Cas** - Liste des cas avec colonnes : ID, Question, R√©ponse attendue, R√©ponse IA, Score (0/0.5/1), Commentaire
2. **Scoring Automatique** - Formules pour calculer score global, par cat√©gorie
3. **Dashboard** - Visualisation graphique du r√©sultat
4. **Instructions** - Comment utiliser le template

---

## Contenus Secondaires

### Checklists

**`checklists/qualite-minimum.md`**
- Ce qu'il FAUT avoir avant de lancer (non n√©gociable)
- Format : checklist avec explication pour chaque point

**`checklists/pre-production.md`**
- V√©rifications avant d√©ploiement en production
- S√©curit√©, performance, monitoring, rollback plan

### Guides Additionnels

**`guides/organiser-tests-utilisateurs.md`**
- Recruter les testeurs
- Pr√©parer le protocole
- Collecter le feedback
- Analyser les r√©sultats

**`guides/criteres-go-production.md`**
- Matrice de d√©cision go/no-go
- Qui d√©cide quoi
- Red flags vs green lights

### Templates Additionnels

**`templates/rapport-qualite.md`**
- Template de rapport mensuel sur la qualit√© IA
- KPIs √† suivre
- Format ex√©cutif pour management

**`templates/feedback-utilisateur.md`**
- Formulaire de satisfaction post-utilisation
- Questions ouvertes et ferm√©es
- √âchelles de notation

### Exemples

**`examples/evaluation-chatbot.md`**
- Cas complet document√©
- Passage Niveau 1 ‚Üí 2 ‚Üí 3
- Donn√©es r√©elles (anonymis√©es)
- D√©cisions prises √† chaque √©tape

**`examples/evaluation-analyse-sentiment.md`**
- Cas d'√©chec instructif
- Pourquoi le projet a √©t√© arr√™t√©
- Le√ßons apprises
- Ce qu'on aurait d√ª faire diff√©remment

---

## Planning d'Ex√©cution

### Semaine 1 (Setup + Core)
- [ ] Cr√©er structure de dossiers
- [ ] README.md avec pr√©sentation LaFabriqAI
- [ ] LICENSE (MIT)
- [ ] CONTRIBUTING.md
- [ ] **Framework 3-niveaux-evaluation.md** (10h) ‚Üê PRIORIT√â 1
- [ ] Checklist qualite-minimum.md (2h)

### Semaine 2 (Contenu Essentiel)
- [ ] **Guide questions-equipe-technique.md** (4h) ‚Üê PRIORIT√â 2
- [ ] **Template grille-evaluation.xlsx** (4h) ‚Üê PRIORIT√â 3
- [ ] Exemple evaluation-chatbot.md (3h)
- [ ] Template feedback-utilisateur.md (2h)

### Semaine 3 (Enrichissement)
- [ ] Guide organiser-tests-utilisateurs.md
- [ ] Guide criteres-go-production.md
- [ ] Checklist pre-production.md
- [ ] Template rapport-qualite.md

### Semaine 4 (Finalisation)
- [ ] Exemple evaluation-analyse-sentiment.md
- [ ] Framework metriques-business-vs-tech.md
- [ ] Framework quand-arreter-projet.md
- [ ] SOURCES.md
- [ ] R√©vision compl√®te et polish

---

## Sp√©cificit√©s README

Le README de ce repo doit mettre en avant :
- Le probl√®me : "92% de pr√©cision" ne veut rien dire pour un manager
- La solution : 3 questions simples dans l'ordre
- Quick Start : liens directs vers les 3 niveaux du framework
- Exemples concrets avec r√©sultats chiffr√©s

---

## Sources et Inspirations

√Ä r√©f√©rencer dans SOURCES.md :
- CGD Framework for AI Evaluation
- Google's Model Cards
- Microsoft's AI Fairness Checklist
- Exp√©riences terrain LaFabriqAI

---

*Ce fichier contient les instructions sp√©cifiques pour le repo ai-evaluation-pratique. Pour les standards g√©n√©raux, voir le fichier `instructions`.*
